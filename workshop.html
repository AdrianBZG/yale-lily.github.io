---
layout: page
title: Workshop
permalink: /workshop/
---


<html>
  <head>
    <title>The Data Science workshop on Computational Social Science</title>
    <style>
<!--
 li.MsoNormal
 {mso-style-parent:"";
 margin-bottom:.0001pt;
 font-size:12.0pt;
 font-family:"Times New Roman";
 margin-left:0cm; margin-right:0cm; margin-top:0cm}
-->
</style>
  </head>

<!--  <body bgcolor="#fff8c6" alink="#5e5a80"> -->

<body alink="#5e5a80">

    <h1 align="center">The Data Science workshop on Computational Social Science</h1>
    <h3 align="center">Friday, October 20, 2017</h3>
    <h3 align="center">Luce Hall, 34 Hillhouse Avenue at Yale University</h3>

<h2>Location</h2>
<p>The workshop will be held at Luce Hall <a href="http://conferencesandevents.yale.edu/campus/venues/luce-hall" target="_blank"></a> at Yale University. The address is 34 Hillhouse Ave, New Haven, CT 06511. </p>
           <iframe width="700" height="350" frameborder="0" scrolling="no" marginheight="0" marginwidth="0" src="https://www.google.com/maps/embed/v1/place?q=place_id:ChIJTxEsvLfZ54kRSmWXc78lmh0&key=AIzaSyC9I1jdJUkkDqPm8OXQlzPQcsVLM5juJkg" allowfullscreen></iframe>

<h2>Announcements</h2>
<ul>
  <li>We now have our list of speakers
  <p>
  <ul>
    <li><a href="http://www.brown.edu/Research/Shapiro/"> <strong>Jesse Shapiro</strong> </a>, Brown</font> </li>
    <li><a href="http://www.jennwv.com"> <strong>Jenn Wortman Vaughn</strong> </a>,  Microsoft Research</font> </li>
    <li><a href="https://ryancotterell.github.io/"> <strong>Ryan Cotterell</strong> </a>, Johns Hopkins</font> </li>
    <li><a href="http://brenocon.com"> <strong>Brendan O'Connor</strong> </a>,  UMass</font> </li>
  </ul>
  <p>
</ul>

<br> <br>

<h2>Schedule (Tentative)</h2>
<table class="table table-striped table-hover">
<tr>
    <th> Time </th> <th> Title </th> <th> Presenter</th> 
</tr>
    <tr>
        <td> TBD </td>
        <td> Welcome and Introduction </td>
        <td> <a href="http://www.cs.yale.edu/homes/radev/"> Dragomir Radev </a> </td>
    </tr>

    <tr>
        <td> TBD </td>
        <td>
        <a href="#mycollapse1" data-toggle="collapse"> Measuring Polarization in High-Dimensional Data: Method and Application to Congressional Speech </a>
        <div style="max-width:400px" id="mycollapse1" class="collapse">
        We study trends in the partisanship of congressional speech from 1873 to 2016.  We define partisanship to be the ease
        with which an observer could infer a congressperson’s party from a fixed amount of speech, and we estimate it using a
        structural choice model and methods from machine learning.  Our method corrects a severe finite-sample bias that we show
        arises with standard estimators. The results reveal that partisanship is far greater in recent years than in the past,
        and that it increased sharply in the early 1990s after remaining low and relatively constant over the preceding
        century. Our method is applicable to the study of high-dimensional choices in many domains, and we illustrate its
        broader utility with an application to residential segregation.
        </div>
        </td>
        <td> <a href="http://www.brown.edu/Research/Shapiro/"> Jesse Shapiro </a> </td>
    </tr>

    <tr>
        <td> TBD </td>
        <td>
        <a href="#mycollapse2" data-toggle="collapse"> The Human Components of Machine Learning </a>
        <div style="max-width:400px" id="mycollapse2" class="collapse">
        Machine learning is often viewed as an automated process.  Data is fed to a learning algorithm that outputs a
        trained model which then makes predictions.  In practice, however it is common for every step of this process to rely on
        humans in the loop.  Training data is often generated through human activity, either as passive observations of social
        processesor actively crowdsourced annotations.  Humans prepare this data for use by the algorithm, engineer features,
        and tweak the algorithm’s parameters to fit their needs.  And in many applications to medicine, criminal justice, and
        other critical domains, humans must interpret the learned model’s predictions to determine how to best make use of these
        predictions in their own decision making process.  In this talk, I’ll argue for the importance of understanding the
        humans in the loop.  As one example, I’ll describe some of my own research on crowdsourcing aimed at understanding who
        the crowd is, what motivates them, and how they communicate.  I’ll also touch on a new direction of research that I’m
        particularly excited about, studying how to make machine learning human-interpretable, and what human interpretability
        even means.
        </div>
        </td>
        <td> <a href="http://www.jennwv.com"> Jenn Wortman Vaughn </a> </td>
    </tr>

    <tr>
        <td> TBD </td>
        <td>
        <a href="#mycollapse3" data-toggle="collapse"> Probabilistic Typology: Deep Generative Models of Vowel Inventories </a>
        <div style="max-width:400px" id="mycollapse3" class="collapse">
        Linguistic typology studies the range of structures present in human language. The main goal of the field is to discover
        which sets of possible phenomena are universal, and which are merely frequent. For example, all languages have vowels,
        while most—but not all—languages have an [u] sound. In this paper we present the first probabilistic treatment of a
        basic question in phonological typology: What makes a natural vowel inventory? We introduce a series of deep stochastic
        point processes, and contrast them with previous computational, simulation-based approaches. We provide a comprehensive
        suite of experiments on over 200 distinct languages.
        </div>
        </td>
        <td> <a href="https://ryancotterell.github.io/"> Ryan Cotterell </a> </td>
    </tr>


    <tr>
        <td> TBD </td>
        <td>
        <a href="#mycollapse4" data-toggle="collapse"> Social Event Extraction: Inferring International Relations and Police Killings from the News </a>
        <div style="max-width:400px" id="mycollapse4" class="collapse">
        What can text analysis tell us about society?  Enormous corpora of news,
        social media, and historical documents record events, beliefs, and
        culture.  Automated text analysis scales to large data sets, and can assist
        in discovering patterns and themes.
        
        I will discuss projects to extract event databases from the news, in the
        domains of international relations and police killings in the U.S.  First,
        we analyze the raw text of 15 years of news articles to extract temporal
        trends of diplomacy, conflict, and military actions between pairs of
        countries, including, for example, the recent history of
        Israeli-Palestinian relations.  Our model combines syntactic parsing and
        latent-variable probabilistic modeling to induce event classes, and we
        validate against predefined ontologies and databases of interstate
        conflict.  Second, we tackle the surprising lack of systematic records on
        police killings of civilians in the U.S., by helping automate the
        extraction of these fatality events from news articles, in order to assist
        manual curation efforts.  Our methods make use of distant supervision and
        outperform extractors used in previous NLP research.
        
        In addition to using natural language processing to advance social
        understanding, findings from the social sciences can better inform the
        design of artificial intelligence.  Given time and interest, I will briefly
        overview our efforts to identify and fix dialectal and racial disparity in
        language technologies.
        </div>
        </td>
        <td> <a href="http://brenocon.com"> Brendan O'Connor </a> </td>
    </tr>


</table>

<style>
#pubTable_filter{
    display:none;
}
</style>

<table id="pubTable" class="table table-hover"></table>
</body>
</html>
