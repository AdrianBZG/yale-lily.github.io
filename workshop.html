---
layout: page
title: Workshop
permalink: /workshop/
---


<html>
  <head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>The Data Science workshop on Computational Social Science</title>
  <style>
  <!--
   li.MsoNormal
   {mso-style-parent:"";
   margin-bottom:.0001pt;
   font-size:12.0pt;
   font-family:"Times New Roman";
   margin-left:0cm; margin-right:0cm; margin-top:0cm}
  -->
  </style>
  </head>

<body alink="#5e5a80">

<h1 align="center">The Data Science workshop on Computational Social Science</h1>
<h3 align="center">Friday, October 20, 2017</h3>
<h3 align="center">Luce Hall, 34 Hillhouse Avenue at Yale University</h3>

<h2>Location</h2>
<p>The workshop will be held at Luce Hall <a href="http://conferencesandevents.yale.edu/campus/venues/luce-hall" target="_blank"></a> at Yale University. The address is 34 Hillhouse Ave, New Haven, CT 06511. </p>
           <iframe width="700" height="350" frameborder="0" scrolling="no" marginheight="0" marginwidth="0" src="https://www.google.com/maps/embed/v1/place?q=place_id:ChIJTxEsvLfZ54kRSmWXc78lmh0&key=AIzaSyC9I1jdJUkkDqPm8OXQlzPQcsVLM5juJkg" allowfullscreen></iframe>

<h2>Announcements</h2>
<ul>
  <li>We now have our list of speakers
  <p>
  <ul>
    <li><a href="http://www.brown.edu/Research/Shapiro/"> <strong>Jesse Shapiro</strong></a>, Brown</font> </li>
    <li><a href="http://www.jennwv.com"> <strong>Jenn Wortman Vaughn</strong></a>, Microsoft Research</font> </li>
    <li><a href="https://ryancotterell.github.io/"> <strong>Ryan Cotterell</strong></a>, Johns Hopkins</font> </li>
    <li><a href="http://brenocon.com"> <strong>Brendan O'Connor</strong></a>, UMass</font> </li>
  </ul>
  <p>
</ul>

<br> <br>

<h2>Schedule (Tentative)</h2>
<table class="table table-striped table-hover">
<tr>
    <th> Time </th> <th> Title </th> <th> Presenter</th> 
</tr>
    <tr>
        <td> TBD </td>
        <td> Welcome and Introduction </td>
        <td> <a href="http://www.cs.yale.edu/homes/radev/"> Dragomir Radev </a> </td>
    </tr>

    <tr>
        <td> TBD </td>
        <td>
        <a href="#mycollapse1" data-toggle="collapse"> Measuring Polarization in High-Dimensional Data: Method and Application to Congressional Speech </a>
        <div style="max-width:400px" id="mycollapse1" class="collapse">
        Abstarct: We study trends in the partisanship of congressional speech from 1873 to 2016.  We define partisanship to be the ease
        with which an observer could infer a congressperson’s party from a fixed amount of speech, and we estimate it using a
        structural choice model and methods from machine learning.  Our method corrects a severe finite-sample bias that we show
        arises with standard estimators. The results reveal that partisanship is far greater in recent years than in the past,
        and that it increased sharply in the early 1990s after remaining low and relatively constant over the preceding
        century. Our method is applicable to the study of high-dimensional choices in many domains, and we illustrate its
        broader utility with an application to residential segregation.

        Bio: Jesse Shapiro is the George S. and Nancy B. Parker Professor of Economics at Brown University. Prior to joining Brown
        University in 2015 he was the Chookaszian Family Professor of Economics at the University of Chicago Booth School of
        Business. Shapiro received his BA in economics in 2001 and his PhD in economics in 2005 from Harvard University. He is a
        Research Associate at the National Bureau of Economic Research and a former editor of the Journal of Political
        Economy. He was a 2011-12 Alfred P. Sloan Research Fellow.
        </div>
        </td>
        <td> <a href="http://www.brown.edu/Research/Shapiro/"> Jesse Shapiro </a> </td>
    </tr>

    <tr>
        <td> TBD </td>
        <td>
        <a href="#mycollapse2" data-toggle="collapse"> The Human Components of Machine Learning </a>
        <div style="max-width:400px" id="mycollapse2" class="collapse">
        Abstract: Machine learning is often viewed as an automated process.  Data is fed to a learning algorithm that outputs a
        trained model which then makes predictions.  In practice, however it is common for every step of this process to rely on
        humans in the loop.  Training data is often generated through human activity, either as passive observations of social
        processesor actively crowdsourced annotations.  Humans prepare this data for use by the algorithm, engineer features,
        and tweak the algorithm’s parameters to fit their needs.  And in many applications to medicine, criminal justice, and
        other critical domains, humans must interpret the learned model’s predictions to determine how to best make use of these
        predictions in their own decision making process.  In this talk, I’ll argue for the importance of understanding the
        humans in the loop.  As one example, I’ll describe some of my own research on crowdsourcing aimed at understanding who
        the crowd is, what motivates them, and how they communicate.  I’ll also touch on a new direction of research that I’m
        particularly excited about, studying how to make machine learning human-interpretable, and what human interpretability
        even means.

        Bio: Jenn Wortman Vaughan is a Senior Researcher at Microsoft Research, New York City.  She studies algorithmic
        economics, machine learning, and social computing, often in the conext of prediction markets, crowdsourcing, and other
        human-in-the-loop systems.  Jenn came to MSR in 2012 from UCLA, where she was an assistant professor in the computer
        science department.  She completed her Ph.D. at the University of Pennsylvania in 2009, and subsequently spent a year as
        a Computing Innovation Fellow at Harvard.  She is the recipient of Penn's 2009Rubinoff dissertation award for innovative
        applications of computer technology, a National Science Foundation CAREER award, a Presidential Early Career Award for
        Scientists and Engineers (PECASE), and a handful of best paper or best student paper awards.  In her "spare" time, Jenn
        is involved in a variety of efforts to provide support for women in computer science; most notably, she co-founded the
        Annual Workshop for Women in Machine Learning, which has been held each year since 2006.
        </div>
        </td>
        <td> <a href="http://www.jennwv.com"> Jenn Wortman Vaughn </a> </td>
    </tr>

    <tr>
        <td> TBD </td>
        <td>
        <a href="#mycollapse3" data-toggle="collapse"> Probabilistic Typology: Deep Generative Models of Vowel Inventories </a>
        <div style="max-width:400px" id="mycollapse3" class="collapse">
        Abstract: Linguistic typology studies the range of structures present in human language. The main goal of the field is to discover
        which sets of possible phenomena are universal, and which are merely frequent. For example, all languages have vowels,
        while most—but not all—languages have an [u] sound. In this paper we present the first probabilistic treatment of a
        basic question in phonological typology: What makes a natural vowel inventory? We introduce a series of deep stochastic
        point processes, and contrast them with previous computational, simulation-based approaches. We provide a comprehensive
        suite of experiments on over 200 distinct languages.

        Bio: Ryan is a fourth year Ph.D. student in the Johns Hopkins Computer Science department affiliated with the Center for
        Language and Speech Processing, where he is coadvised by Jason Eisner and David Yarowsky. He specializes in natural
        language processing, computational linguistics and machine learning, focusing on deep learning and statistical
        approaches to phonology, morphology, linguistic typology and low-resource languages. He has received best paper awards
        at ACL 2017 and EACL 2017 and two honorable mentions for best paper at EMNLP 2015 and NAACL 2016. Previously, he was a
        visiting Ph.D. student at the Center for Information and Language Processing at LMU Munich supported by a Fulbright
        Fellowship and a DAAD Research Grant under the supervision of Hinrich Schütze. Since Fall 2016 he has been supported by
        an NDSEG graduate fellowship and since 2017 by the Fredrick Jelinek Fellowship.
        </div>
        </td>
        <td> <a href="https://ryancotterell.github.io/"> Ryan Cotterell </a> </td>
    </tr>


    <tr>
        <td> TBD </td>
        <td>
        <a href="#mycollapse4" data-toggle="collapse"> Social Event Extraction: Inferring International Relations and Police Killings from the News </a>
        <div style="max-width:400px" id="mycollapse4" class="collapse">
        Abstract: What can text analysis tell us about society?  Enormous corpora of news,
        social media, and historical documents record events, beliefs, and
        culture.  Automated text analysis scales to large data sets, and can assist
        in discovering patterns and themes.
        
        I will discuss projects to extract event databases from the news, in the
        domains of international relations and police killings in the U.S.  First,
        we analyze the raw text of 15 years of news articles to extract temporal
        trends of diplomacy, conflict, and military actions between pairs of
        countries, including, for example, the recent history of
        Israeli-Palestinian relations.  Our model combines syntactic parsing and
        latent-variable probabilistic modeling to induce event classes, and we
        validate against predefined ontologies and databases of interstate
        conflict.  Second, we tackle the surprising lack of systematic records on
        police killings of civilians in the U.S., by helping automate the
        extraction of these fatality events from news articles, in order to assist
        manual curation efforts.  Our methods make use of distant supervision and
        outperform extractors used in previous NLP research.
        
        In addition to using natural language processing to advance social
        understanding, findings from the social sciences can better inform the
        design of artificial intelligence.  Given time and interest, I will briefly
        overview our efforts to identify and fix dialectal and racial disparity in
        language technologies.

        Bio: Brendan O'Connor (http://brenocon.com/) is an assistant professor in the
        College of Information and Computer Sciences at the University of
        Massachusetts, Amherst. Prof. O'Connor works in computational social
        science, developing natural language processing, machine learning, and
        interface tools to help scientific investigation about political and social
        trends; for example, analyzing opinions and slang in Twitter, censorship in
        Chinese microblogs, and and political events reported in the news. His work
        has been featured in the New York Times and the Wall Street Journal. He
        received his PhD in 2014 from Carnegie Mellon University's Machine Learning
        Department, advised by Noah A. Smith, and has previously been a Visiting
        Fellow at the Harvard Institute for Quantitative Social Science, and an
        intern with the Facebook Data Science team. Before graduate school, he
        worked on crowdsourcing at CrowdFlower / Dolores Labs, and natural language
        search at Powerset. He holds an BS and MS in Symbolic Systems from Stanford
        University.
        </div>
        </td>
        <td> <a href="http://brenocon.com"> Brendan O'Connor </a> </td>
    </tr>


</table>

<style>
#pubTable_filter{
    display:none;
}
</style>

<table id="pubTable" class="table table-hover"></table>
</body>
</html>
